# -*- coding: utf-8 -*-
"""Copy of PPO final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QotfBJuqoixL7raCYkZc0ZLlQKwmj6SI
"""

import multiprocessing
import multiprocessing.connection
import time
from collections import deque
from typing import Dict, List

import cv2
import gym
import numpy as np
import torch
from torch import nn
from torch import optim
from torch.distributions import Categorical
from torch.nn import functional as F
from tempfile import TemporaryFile

if torch.cuda.is_available(): #cuda gpu stuff, unclear how this will work on colab
    device = torch.device("cuda:0")
else:
    device = torch.device("cpu")
import datetime
import pickle
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive')

"""The Game class acts as a wrapper for the environment. Instead of using just the default gym, this allows easy conversion when needed into our processed screen image and allows for easily doing multiple steps at once."""

class Game:
  def __init__(self, seed):


        self.env = gym.make('MsPacman-ram-v0')
        self.env.seed(seed)
        self.obs_all_4 = np.zeros((128, 4))
        self.rewards = []
        self.frames = []
        self.check_frames = seed % 8 == 0
        

  def step(self, action):
        reward = 0.
        done = None
        obs = None
        for i in range(4): # take 4 steps with the same action
            obs, r, done, info = self.env.step(action)
            if self.check_frames:
              self.frames.append(self.env.unwrapped._get_image())
            reward += r
            if done: # finish after all 3 lives are lost
                break
        self.rewards.append(reward)

        if done: #Store relevant info and return

            episode_info = {"reward": sum(self.rewards),
                            "length": len(self.rewards),
                            "frames": self.frames}
            self.reset()
        else: #Adds final frame to rolling set of 4
            episode_info = None

            obs = self._process_obs(obs) 

            self.obs_all_4 = np.roll(self.obs_all_4, shift=-1, axis=-1)
            self.obs_all_4[..., -1:] = obs #adds obs to the end

        return self.obs_all_4, reward, done, episode_info

  def reset(self): # resets relevant variables when a run is completed

        obs = self.env.reset()

        obs = self._process_obs(obs)
        self.obs_all_4[..., 0:] = obs
        self.obs_all_4[..., 1:] = obs
        self.obs_all_4[..., 2:] = obs
        self.obs_all_4[..., 3:] = obs
        self.rewards = []
        self.frames = []

        return self.obs_all_4

  @staticmethod #Im trying to be a fancy programmer
  def _process_obs(obs): #Attempt to reduce noise on the screen
        return obs.reshape(128, 1)

def worker_process(remote, seed): #Determines what action the thread should do


    game = Game(seed)

    while True:
        cmd, data = remote.recv()
        if cmd == "step":
            remote.send(game.step(data))
        elif cmd == "reset":
            remote.send(game.reset())
        elif cmd == "close":
            remote.close()
            break
        elif cmd == "save video":
            make_video(data)
        #PLOTTING STUFF
        elif cmd == "save lists":
            save_lists(data)
        elif cmd == "save video":
            make_video(data)
        else:
            raise NotImplementedError

class Worker: #There will be X workers that run through the sampling and 1 that saves the video

    def __init__(self, seed):
        self.child, parent = multiprocessing.Pipe()
        self.process = multiprocessing.Process(target=worker_process, args=(parent, seed))
        self.process.start()

class Model(nn.Module): # The model will have convolutional layers to understand the image then linear layers
    def __init__(self):
        super().__init__()
        
        self.conv1 = nn.Conv1d(4, 32, kernel_size=8, stride=4, padding=0)
        '''
        self.conv2 = nn.Conv1d(32, 64, kernel_size=4,stride=2,padding=0)
        self.conv3 = nn.Conv1d(64, 64,kernel_size=3, stride=1, padding=0)
        '''
        self.lin1 = nn.Linear(992, 128)
        self.lin2 = nn.Linear(128, 32)
        self.pi_logits = nn.Linear(32,  5) # we allow the 4 directions and the stand still action
        self.value = nn.Linear(32, 1)

        #initialize values to try and speed up performance
        '''
        nn.init.orthogonal_(self.conv1.weight, np.sqrt(2))
        nn.init.orthogonal_(self.conv2.weight, np.sqrt(2))
        nn.init.orthogonal_(self.conv3.weight, np.sqrt(2))
        '''
        nn.init.orthogonal_(self.lin1.weight, np.sqrt(2))
        nn.init.orthogonal_(self.pi_logits.weight, np.sqrt(0.01))
        nn.init.orthogonal_(self.value.weight, 1)
        
    def forward(self, obs): #standard with relu except we have 2 returns for pi and value
        h: torch.Tensor
        h = F.relu(self.conv1(obs))
        '''
        h = F.relu(self.conv2(h))
        
        h = F.relu(self.conv3(h))
        '''
        h = h.reshape((-1, 992))
        h = F.relu(self.lin1(h))
        h = F.relu(self.lin2(h))
        pi = Categorical(logits=self.pi_logits(h))
        value = self.value(h).reshape(-1)
        return pi, value
def obs_to_torch(obs):
    '''
    N: number of images in the batch
    H: height of the image
    W: width of the image
    C: number of channels of the image (ex: 3 for RGB, 1 for grayscale...)
    Convert from  [N, H, W, C] to [N, C, H, W] and convert to tensor
    '''

    obs = np.swapaxes(obs, 1, 2)
    #obs = np.swapaxes(obs, 3, 2)

    return torch.tensor(obs, dtype=torch.float32, device=device) / 255.

class Trainer:
    def __init__(self, model): 

        self.model = model
        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)
        #PLOTTING STUFF
        self.losses = []
        
    def train(self, samps, learning_rate, clip_range): #this is where things get a little crazy
        
        """
        Main idea from openai.com
        We want to minimize an objective function that has clipped ranges
        """


        samp_action = samps['actions'] #These are actions sampled using old model
        
        samp_value = samps['vals'] #These are the estimates of the values of those actions
        samp_return = samps['vals'] + samps['advantages'] #The returns from using these actions
        samp_obs = samps['obs'] #These are the states returned from feeding the actions into the model
        samp_normalized_advantage = Trainer._normalize(samps['advantages']) #normalize the sampled advantages
        pi, value = self.model(samp_obs) #this gets the logits and the value
        samp_neg_log_pi = samps['neg_log_pis'] # These are the negative log probabilities of those actions on old model
        neg_log_pi = -pi.log_prob(samp_action) #negative log probabilites on new model
        ratio = torch.exp(samp_neg_log_pi - neg_log_pi) #use the exp function on difference between

        clip_boundaries = ratio.clamp(min=1.0 - clip_range, max=1.0 + clip_range) #get boundaries so modifications to model are bounded
        #if clip is too large, performance can become bad as you begin to sample from a bad model
        policy_reward = torch.min(ratio * samp_normalized_advantage, clip_boundaries * samp_normalized_advantage)
        policy_reward = policy_reward.mean() #we use these clips to determine the reward. 
        #using the clip keeps kl divergance constrained
        
        bonus_from_entropy = pi.entropy()
        bonus_from_entropy = bonus_from_entropy.mean() #we must have some entropy to excourage exploration
        
        value_after_clip = samp_value + (value - samp_value).clamp(min=-clip_range, max=clip_range)
        loss_vf = torch.max((value - samp_return) ** 2, (value_after_clip - samp_return) ** 2)
        loss_vf = 0.5 * loss_vf.mean() #The clip comes in again to constrain the possible change of the value function

        loss = -(policy_reward - 0.5 * loss_vf + 0.01 * bonus_from_entropy) #take negative of the loss as we are maximizing

        #PLOTTING STUFF
        self.losses.append(loss.detach())

        for param_group in self.optimizer.param_groups: #this gets the gradients and applies the optimizer
            param_group['lr'] = learning_rate
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
        self.optimizer.step()

        kl_divergence = .5 * ((neg_log_pi - samp_neg_log_pi) ** 2).mean() 
        fraction = (abs((ratio - 1.0)) > clip_range).type(torch.FloatTensor).mean()

        return [policy_reward,
                loss_vf,
                bonus_from_entropy,
                kl_divergence,
                fraction]
    @staticmethod
    def _normalize(adv: np.ndarray):
        return (adv - adv.mean()) / (adv.std() + 1e-8)

class Main(object):
    def __init__(self):


        self.gamma = 0.99 #these are for calculating the advantage
        self.lamda = 0.95

        self.updates = 5001 #number of times to run the main loop
        self.epochs = 2 #number of times to train the model using the sampled data in the main loop
        self.n_workers = 8 #nubmer of threads
        self.worker_steps = 256 #number of steps each worker should run for each update
        self.n_mini_batch = 4 
        self.lr = 5e-4 #learning rate
        self.clip_range = 0.1 # 1+- clip_range = possible change
        self.batch_size = self.n_workers * self.worker_steps #total size of each batch
        self.mini_batch_size = self.batch_size // self.n_mini_batch #total size of each mini-batch


        self.workers = [Worker(42 + i) for i in range(self.n_workers)] #sample workers
        self.saver = Worker(500) #save worker
        self.obs = np.zeros((self.n_workers, 128, 4), dtype=np.uint8)
        for worker in self.workers:
            worker.child.send(("reset", None))
        for i, worker in enumerate(self.workers):
            self.obs[i] = worker.child.recv()

        self.model = Model()
        self.model.to(device) #cuda stuff

        self.trainer = Trainer(self.model)
        #PLOTTING STUFF
        #comment out either the section immediately below, or the section below that
        self.rewards_plt = []
        self.lengths_plt = []
        self.losses_plt = []
        '''
        with open('gdrive/My Drive/Colab Notebooks/Pacman/' + 'reward_list', 'rb') as fp:
            self.rewards_plt = pickle.load(fp)
        with open('gdrive/My Drive/Colab Notebooks/Pacman/' + 'length_list', 'rb') as fp:
            self.lengths_plt = pickle.load(fp)
        with open('gdrive/My Drive/Colab Notebooks/Pacman/' + 'losses_list', 'rb') as fp:
            self.losses_plt = pickle.load(fp)
        print(self.rewards_plt)
        '''

    def run_training_loop(self):
        episode_info = deque(maxlen=100) #basically a list-like queue that makes sticking new things in easy
        i = 0
        info = 0

        for update in range(self.updates):
            time_start = time.time() #keep track of runtime
            progress = update / self.updates

            samples, info = self.sample() #runs the sample function which starts the threads
            
            self.train(samples, self.lr * (1-progress), self.clip_range * (1-progress)) #trains using results of samples

            time_end = time.time()  

            episode_info.extend(info)
            reward_mean, length_mean = Main._get_mean_(episode_info)
            #PLOTTING STUFF
            self.rewards_plt.append(reward_mean)
            self.lengths_plt.append(length_mean)
            self.losses_plt.append(np.mean(self.trainer.losses))
            print(np.mean(self.trainer.losses))
            self.trainer.losses = []
            if i % 100 == 0: #save on every
              print("rewards", self.rewards_plt)
              self.saver.child.send(("save lists", (self.rewards_plt, self.lengths_plt, self.losses_plt)))
              for info in episode_info:
                if (len(info["frames"])) > 0:
                  self.saver.child.send(("save video", (info['frames'], self.model)))
                  break
            seconds = int((time_end - time_start))
            print(f"{update:4}: seconds to run ={seconds:3} avg reward ={reward_mean:.2f} avg length in frames={length_mean:.3f}")
            i+=1
    def _calc_advantages(self, finished, rewards, vals):#get the advantage for each worker step


        advantages = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
        final_advantage = 0
        _, final_value = self.model(obs_to_torch(self.obs))
        final_value = final_value.cpu().data.numpy()
        #we use generalized advantage estimation
        #this is A_t = delta + gamma * lambda * A_t+1 (so we go in reverse order)
        for t in reversed(range(self.worker_steps)): #go in reverse order of frames to go from end of run to beginnning

            m = 1.0 - finished[:, t] # we want to mask the reult if the episode was finished after this point
            final_value = final_value * m #if finished set val to 0
            final_advantage = final_advantage * m #same here

            delta = rewards[:, t] + self.gamma * final_value - vals[:, t]

            final_advantage = delta + self.gamma * self.lamda * final_advantage
            advantages[:, t] = final_advantage

            final_value = vals[:, t]

        return advantages
    def sample(self):
        #initilize the important arrays
        obs = np.zeros((self.n_workers, self.worker_steps, 128, 4), dtype=np.uint8)
        rewards = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
        actions = np.zeros((self.n_workers, self.worker_steps), dtype=np.int32)
        neg_log_pis = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
        vals = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
        finished = np.zeros((self.n_workers, self.worker_steps), dtype=np.bool)
        
        episode_infos = []

        for t in range(self.worker_steps): #for every step in a single update

            obs[:, t] = self.obs #this stores the last ovservation of each worker since they continue
            pi, v = self.model(obs_to_torch(self.obs)) #we sample possible actions for each worker from the possible 5
            vals[:, t] = v.cpu().data.numpy()
            a = pi.sample()
            actions[:, t] = a.cpu().data.numpy()
            neg_log_pis[:, t] = -pi.log_prob(a).cpu().data.numpy()

            for w, worker in enumerate(self.workers): #run them on each worker
                worker.child.send(("step", actions[w, t]))

            for w, worker in enumerate(self.workers):

                self.obs[w], rewards[w, t], finished[w, t], info = worker.child.recv() #get the results

                if info:
                    info['obs'] = obs[w, t, :, 3]
                    episode_infos.append(info)

        advantages = self._calc_advantages(finished, rewards, vals) #calculate teh advantages
        samples = {
            'obs': obs,
            'actions': actions,
            'vals': vals,
            'neg_log_pis': neg_log_pis,
            'advantages': advantages
        }

        samples_flat = {} #flatten them
        for k, v in samples.items():
            v = v.reshape(v.shape[0] * v.shape[1], *v.shape[2:])
            if k == 'obs':
                samples_flat[k] = obs_to_torch(v)
            else:
                samples_flat[k] = torch.tensor(v, device=device)

        return samples_flat, episode_infos


    def train(self, samples, learning_rate, clip_range): #innner training loop after sampling

        info = []
        for i in range(self.epochs):

            indices = torch.randperm(self.batch_size) #shuffel

            for start in range(0, self.batch_size, self.mini_batch_size): #go for the entire batch with steps of size of each mini batch
                #essentially this is for each mini-batch
                end = start + self.mini_batch_size
                mini_batch_indices = indices[start: end]
                mini_batch = {}
                for k, v in samples.items():
                    mini_batch[k] = v[mini_batch_indices]

                train_values = self.trainer.train(mini_batch, learning_rate, clip_range) #train using this date

                info.append(train_values)

        return np.mean(info, axis=0) #return the average information

        
    @staticmethod
    def _get_mean_(episode_info): #this just gets the mean of the info for reporting purposes
        if len(episode_info) > 0:
            return (np.mean([info["reward"] for info in episode_info]),
                    np.mean([info["length"] for info in episode_info]))
        else:
            return np.nan, np.nan

    def destroy(self): #kills the threads after it finishes
        for worker in self.workers:
            worker.child.send(("close", None))
        self.saver.child.send(("close", None))
    #PLOTTING STUFF
    def plot_graphs(self):
        plt.plot(self.rewards_plt)
        plt.xlabel("Episode")
        plt.ylabel("Reward")
        plt.title("Rewards")
        plt.show()
        plt.close()
        plt.plot(self.lengths_plt)
        plt.xlabel("Episode")
        plt.ylabel("Length")
        plt.title("Lengths")
        plt.show()
        plt.close()
        plt.plot(self.losses_plt)
        plt.xlabel("Episode")
        plt.ylabel("Loss")
        plt.title("Losses")
        plt.show()
        plt.close()

def make_video( info):
          episode_info, model  = info
          fourcc = cv2.VideoWriter_fourcc(*'XVID')

          vw = cv2.VideoWriter('gdrive/My Drive/Colab Notebooks/Pacman2/' + "Reward_number_" + str(datetime.datetime.now()) +  '.avi', fourcc, 4, (160,210))
          for i in range(len(episode_info)):
            if i % 2 == 0:
                vw.write(episode_info[i])
          vw.release()
          torch.save(model, 'gdrive/My Drive/Colab Notebooks/Pacman2/' + "Reward_number_"  + "_Date_" + str(datetime.datetime.now()) + '.pt')
          return "done"
#PLOTTING STUFF
def save_lists(lists_tuple):
    reward_list, length_list, losses_list = lists_tuple
    with open('gdrive/My Drive/Colab Notebooks/Pacman/' + 'reward_list', 'wb') as fp:
        pickle.dump(reward_list, fp)
    with open('gdrive/My Drive/Colab Notebooks/Pacman/' + 'length_list', 'wb') as fp:
        pickle.dump(length_list, fp)
    with open('gdrive/My Drive/Colab Notebooks/Pacman/' + 'losses_list', 'wb') as fp:
        pickle.dump(losses_list, fp)

if __name__ == "__main__":
    m = Main()
    m.run_training_loop()
    m.plot_graphs()
    m.destroy()